# coding=utf-8
"""
å¤–éƒ¨æƒå¨æ¦œå•ï¼ˆæ¯å‘¨å‘¨æŠ¥ç”¨ï¼‰

ç›®æ ‡ï¼š
- ä¸ä½¿ç”¨æœ¬é¡¹ç›®å†…éƒ¨çš„â€œæ ‡é¢˜å‘½ä¸­ç»Ÿè®¡â€æ¥ç”Ÿæˆçƒ­åº¦æ¦œï¼ˆé¿å…å£å¾„äº‰è®®ï¼‰
- ä½¿ç”¨å¤–éƒ¨å…¬å¼€/æƒå¨æ•°æ®æºï¼š
  - OpenRouter Rankingsï¼šæ¨¡å‹ä½¿ç”¨é‡ï¼ˆtoken usageï¼‰+ shareï¼ˆ%ï¼‰
  - LMArenaï¼ˆHuggingFace Space å…¬å¸ƒ CSVï¼‰ï¼šArena-Hard-Auto åŸºå‡†åˆ†ï¼ˆèƒ½åŠ›æ¦œï¼‰
  - PyPI / npmï¼šæœ€è¿‘ä¸€å‘¨ä¸‹è½½é‡ï¼ˆå¼€å‘è€…é‡‡ç”¨åº¦ï¼‰

é‡è¦åŸåˆ™ï¼š
- æ•°æ®æºä¸å¯ç”¨æ—¶ï¼šè·³è¿‡è¯¥åˆ†æ¦œï¼Œä¸é˜»å¡å‘¨æŠ¥ä¸»é“¾è·¯
"""

from __future__ import annotations

import csv
import io
import re
import time
import codecs
from dataclasses import dataclass
from pathlib import Path
from typing import Any, Dict, List, Optional, Tuple

import requests
import yaml


_UA = "TrendRadar/weekly-leaderboard (+https://github.com/)"


@dataclass
class RankedItem:
    rank: int
    name: str
    metric: str
    url: str = ""
    extra: Optional[str] = None


def _http_get_text(url: str, timeout: int = 20) -> str:
    resp = requests.get(
        url,
        timeout=timeout,
        headers={"User-Agent": _UA, "Accept": "text/html,application/json;q=0.9,*/*;q=0.8"},
    )
    resp.raise_for_status()
    return resp.text or ""


def _http_get_json(url: str, timeout: int = 20) -> Any:
    resp = requests.get(url, timeout=timeout, headers={"User-Agent": _UA, "Accept": "application/json"})
    resp.raise_for_status()
    return resp.json()


def load_external_leaderboards_config(
    project_root: str,
    rel_path: str = "config/external_leaderboards.yaml",
) -> Dict[str, Any]:
    path = Path(project_root) / rel_path
    if not path.exists():
        raise FileNotFoundError(f"å¤–éƒ¨æ¦œå•é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {path}")
    data = yaml.safe_load(path.read_text(encoding="utf-8")) or {}
    return data.get("external_leaderboards") or {}


def _parse_openrouter_top_models(page_text: str, top_n: int) -> List[RankedItem]:
    """
    è§£æ https://openrouter.ai/rankings é¡µé¢ä¸­çš„ Top Models åˆ—è¡¨ã€‚

    å…¸å‹ç»“æ„ï¼ˆæ–‡æœ¬åŒ–åï¼‰ï¼š
    1.
    [Claude Sonnet 4.5](https://openrouter.ai/...)
    by[anthropic](...)
    766Btokens
    15%
    """
    items: List[RankedItem] = []

    # ä¼˜å…ˆå°è¯•è§£æâ€œå±•ç¤ºæ–‡æœ¬â€ï¼ˆæŸäº›ç¯å¢ƒä¼šè¿”å›å¯è¯»ç‰ˆæœ¬ï¼‰
    pat = re.compile(
        r"(\d+)\.\s*"
        r"(?:!\[[^\]]*\]\([^)]+\)\s*)?"
        r"\[(?P<name>[^\]]+)\]\((?P<url>https?://openrouter\.ai/[^)]+)\)\s*"
        r"by\[(?P<author>[^\]]+)\]\([^)]+\)\s*"
        r"(?P<tokens>[\d.]+[KMBT]?)tokens\s*"
        r"(?P<share>[\d.]+%)",
        re.IGNORECASE | re.DOTALL,
    )
    for m in pat.finditer(page_text):
        rank = int(m.group(1))
        name = (m.group("name") or "").strip()
        url = (m.group("url") or "").strip()
        tokens = (m.group("tokens") or "").strip()
        share = (m.group("share") or "").strip()
        author = (m.group("author") or "").strip()
        if not name or not tokens or not share:
            continue
        metric = f"{tokens} tokens | {share}"
        extra = f"by {author}" if author else None
        items.append(RankedItem(rank=rank, name=name, metric=metric, url=url, extra=extra))
        if len(items) >= top_n:
            return items

    # å›é€€ï¼šä» Next.js RSC æ³¨å…¥çš„â€œæ¨¡å‹ç»Ÿè®¡æ•°ç»„â€æå– request_countï¼ˆä»æ˜¯ OpenRouter å®˜æ–¹æ•°æ®ï¼‰
    # å½¢å¼ï¼š[{\"id\":\"...\",\"slug\":\"...\",\"name\":\"...\",\"author\":\"...\",\"request_count\":...}, ...]
    try:
        start_positions = [m.start() for m in re.finditer(r"\[\{\\\"id\\\":\\\"", page_text)]
        for start in start_positions:
            window = page_text[start : start + 2000]
            if "request_count" not in window:
                continue

            frag = page_text[start : start + 80000]
            level = 0
            end = None
            for i, ch in enumerate(frag):
                if ch == "[":
                    level += 1
                elif ch == "]":
                    level -= 1
                    if level == 0:
                        end = i + 1
                        break
            if not end:
                continue
            arr_esc = frag[:end]
            arr_json = arr_esc.replace("\\\"", "\"")
            models = __import__("json").loads(arr_json)
            if not isinstance(models, list) or not models:
                continue
            models = [m for m in models if isinstance(m, dict) and m.get("request_count")]
            models.sort(key=lambda x: int(x.get("request_count") or 0), reverse=True)
            ranked: List[RankedItem] = []
            for idx, mobj in enumerate(models[:top_n], 1):
                name = str(mobj.get("name") or "").strip()
                slug = str(mobj.get("slug") or "").strip()
                req = int(mobj.get("request_count") or 0)
                if not name or req <= 0:
                    continue
                url = f"https://openrouter.ai/{slug}" if slug else ""
                ranked.append(RankedItem(rank=idx, name=name, metric=f"{req:,} requests", url=url))
            return ranked
    except Exception:
        return []

    return []


def _parse_openrouter_top_apps(page_text: str, top_n: int) -> List[RankedItem]:
    """
    è§£æ https://openrouter.ai/rankings/apps é¡µé¢ä¸­çš„ Top Apps åˆ—è¡¨ã€‚

    å…¸å‹ç»“æ„ï¼ˆæ–‡æœ¬åŒ–åï¼‰ï¼š
    1.
    [liteLLM](https://openrouter.ai/apps?url=...)
    Open-source library to simplify LLM calls
    66.9Btokens
    """
    items: List[RankedItem] = []
    # ä» â€œ## Top Appsâ€ å¼€å§‹æˆªæ–­ï¼Œé¿å…å‰é¢çš„æ¨¡å‹æ¦œå¹²æ‰°
    idx = page_text.find("## Top Apps")
    text = page_text[idx:] if idx >= 0 else page_text

    pat = re.compile(
        r"(\d+)\.\s*"
        r"(?:!\[[^\]]*\]\([^)]+\)\s*)?"
        r"\[(?P<name>[^\]]+)\]\((?P<url>https?://openrouter\.ai/apps\?url=[^)]+)\)\s*"
        r"(?P<desc>.*?)\s*"
        r"(?P<tokens>[\d.]+[KMBT]?)tokens",
        re.IGNORECASE | re.DOTALL,
    )
    for m in pat.finditer(text):
        rank = int(m.group(1))
        name = (m.group("name") or "").strip()
        url = (m.group("url") or "").strip()
        desc = (m.group("desc") or "").strip().replace("\n", " ")
        tokens = (m.group("tokens") or "").strip()
        if not name:
            continue
        items.append(RankedItem(rank=rank, name=name, metric=f"{tokens} tokens", url=url, extra=desc or None))
        if len(items) >= top_n:
            return items

    # å›é€€ï¼šè§£æ Next.js RSC æ³¨å…¥çš„ rankMapï¼ˆåŒ…å« day/week/month çš„ app total_tokensï¼‰
    try:
        push_re = re.compile(r'self\.__next_f\.push\(\[1,"(?P<payload>(?:\\.|[^"])*)"\]\)')
        payload_with_rankmap = None
        for mm in push_re.finditer(page_text):
            payload = mm.group("payload")
            if "rankMap" in payload:
                payload_with_rankmap = payload
                break
        if not payload_with_rankmap:
            return []

        decoded = codecs.decode(payload_with_rankmap, "unicode_escape")
        pos_candidates = [p for p in (decoded.find("["), decoded.find("{")) if p != -1]
        if not pos_candidates:
            return []
        js = decoded[min(pos_candidates) :]
        data = __import__("json").loads(js)

        rankmap = None
        stack = [data]
        while stack:
            cur = stack.pop()
            if isinstance(cur, dict):
                if "rankMap" in cur and isinstance(cur["rankMap"], dict):
                    rankmap = cur["rankMap"]
                    break
                stack.extend(cur.values())
            elif isinstance(cur, list):
                stack.extend(cur)

        if not rankmap:
            return []
        period = "week" if "week" in rankmap else ("day" if "day" in rankmap else "month")
        arr = rankmap.get(period) or []
        if not isinstance(arr, list) or not arr:
            return []

        def _fmt_tokens(n: int) -> str:
            for unit, div in (("T", 10**12), ("B", 10**9), ("M", 10**6), ("K", 10**3)):
                if n >= div:
                    return f"{n/div:.1f}{unit} tokens"
            return f"{n} tokens"

        ranked: List[RankedItem] = []
        for i, entry in enumerate(arr[:top_n], 1):
            if not isinstance(entry, dict):
                continue
            app = entry.get("app") or {}
            title = str(app.get("title") or "").strip()
            if not title:
                continue
            total_tokens_str = str(entry.get("total_tokens") or "0")
            try:
                total_tokens = int(total_tokens_str)
            except Exception:
                total_tokens = 0
            url = str(app.get("origin_url") or app.get("main_url") or "").strip()
            ranked.append(RankedItem(rank=i, name=title, metric=_fmt_tokens(total_tokens), url=url, extra=app.get("description") or None))
        return ranked
    except Exception:
        return []


def fetch_openrouter_rankings(
    *,
    url_models: str,
    url_apps: str,
    include_apps: bool,
    top_n: int,
) -> Tuple[List[RankedItem], List[RankedItem]]:
    models: List[RankedItem] = []
    apps: List[RankedItem] = []

    # åŒä¸€ä¸ªé¡µé¢é‡Œé€šå¸¸åŒ…å«æ¨¡å‹ç»Ÿè®¡ï¼ˆrequest_countï¼‰ä¸ apps rankMapï¼ˆtotal_tokensï¼‰ã€‚
    try:
        text = _http_get_text(url_models)
        models = _parse_openrouter_top_models(text, top_n=top_n)
        # apps çš„ rankMap ä¹Ÿåœ¨ rankings é¡µé‡Œï¼Œä¼˜å…ˆç”¨åŒä¸€ä»½ HTMLï¼Œé¿å…é‡å¤è¯·æ±‚
        if include_apps:
            apps = _parse_openrouter_top_apps(text, top_n=top_n)
    except Exception:
        models = []
        apps = []

    # è‹¥éœ€è¦ apps ä¸”æœªå–åˆ°ï¼Œå†å°è¯• apps é¡µé¢
    if include_apps and not apps:
        try:
            text = _http_get_text(url_apps)
            apps = _parse_openrouter_top_apps(text, top_n=top_n)
        except Exception:
            apps = []

    return models, apps


def fetch_lmarena_arena_hard_auto(
    *,
    csv_url: str,
    top_n: int,
) -> Tuple[List[RankedItem], Optional[str]]:
    """
    è¿”å› (æ¦œå•, date_str)
    """
    try:
        csv_text = _http_get_text(csv_url)
    except Exception:
        return [], None

    rows: List[Dict[str, str]] = []
    reader = csv.DictReader(io.StringIO(csv_text))
    for row in reader:
        if not row:
            continue
        rows.append({k: (v or "").strip() for k, v in row.items()})

    if not rows:
        return [], None

    # å°è¯•æå– dateï¼ˆæ–‡ä»¶é‡Œé€šå¸¸ç›¸åŒï¼‰
    date_str = None
    for r in rows:
        if r.get("date"):
            date_str = r.get("date")
            break

    def _score(row: Dict[str, str]) -> float:
        try:
            return float(row.get("score", "") or 0)
        except Exception:
            return 0.0

    rows.sort(key=lambda r: _score(r), reverse=True)
    items: List[RankedItem] = []
    for i, r in enumerate(rows[:top_n], 1):
        name = r.get("model", "") or ""
        score = r.get("score", "") or ""
        ci = r.get("CI", "") or ""
        metric = f"score {score}"
        extra = f"CI {ci}" if ci else None
        items.append(RankedItem(rank=i, name=name, metric=metric, extra=extra))
    return items, date_str


def fetch_pypi_downloads_last_week(package: str, timeout: int = 20) -> Optional[int]:
    """
    pypistats: https://pypistats.org/api/packages/<package>/recent?period=week
    """
    url = f"https://pypistats.org/api/packages/{package}/recent?period=week"
    backoff = 1.0
    for _ in range(3):
        try:
            resp = requests.get(url, timeout=timeout, headers={"User-Agent": _UA, "Accept": "application/json"})
            if resp.status_code == 429:
                retry_after = resp.headers.get("Retry-After")
                if retry_after and retry_after.isdigit():
                    time.sleep(min(10, int(retry_after)))
                else:
                    time.sleep(min(10, backoff))
                    backoff *= 2
                continue
            resp.raise_for_status()
            data = resp.json()
            val = (data or {}).get("data", {}).get("last_week")
            if isinstance(val, int):
                return val
            if isinstance(val, float):
                return int(val)
            return None
        except Exception:
            time.sleep(min(5, backoff))
            backoff *= 2
            continue
    return None


def fetch_npm_downloads_last_week(package: str, timeout: int = 20) -> Optional[int]:
    """
    npm downloads API:
    https://api.npmjs.org/downloads/point/last-week/<package>
    """
    try:
        data = _http_get_json(f"https://api.npmjs.org/downloads/point/last-week/{package}", timeout=timeout)
        val = (data or {}).get("downloads")
        if isinstance(val, int):
            return val
        if isinstance(val, float):
            return int(val)
        return None
    except Exception:
        return None


def _render_ranked_list(items: List[RankedItem]) -> List[str]:
    lines: List[str] = []
    for it in items:
        if it.url:
            lines.append(f"{it.rank}. **[{it.name}]({it.url})** â€” {it.metric}" + (f"ï¼ˆ{it.extra}ï¼‰" if it.extra else ""))
        else:
            lines.append(f"{it.rank}. **{it.name}** â€” {it.metric}" + (f"ï¼ˆ{it.extra}ï¼‰" if it.extra else ""))
    return lines


def render_weekly_external_leaderboards_markdown(
    *,
    project_root: str,
    top_n: int = 10,
) -> str:
    cfg = load_external_leaderboards_config(project_root)
    top_n = max(1, int(cfg.get("top_n", top_n) or top_n))

    lines: List[str] = []
    lines.append("## ğŸ“ˆ AIæ¨¡å‹/å·¥å…·çƒ­åº¦æ’è¡Œæ¦œï¼ˆå¤–éƒ¨æƒå¨å£å¾„ï¼‰")
    lines.append("")
    lines.append("> è¯´æ˜ï¼šæœ¬åŒºå—æ¥è‡ªå¤–éƒ¨å…¬å¼€æ•°æ®æºï¼ˆOpenRouter / LMArena / PyPI / npmï¼‰ï¼Œç”¨äºâ€œè¡Œä¸šçƒ­åº¦/é‡‡ç”¨åº¦â€å‚è€ƒï¼›è‹¥æŸæºä¸å¯ç”¨å°†è‡ªåŠ¨è·³è¿‡ï¼Œä¸å½±å“å‘¨æŠ¥ç”Ÿæˆã€‚")
    lines.append("")

    # ===== æ¨¡å‹æ¦œï¼šOpenRouter =====
    models_cfg = (cfg.get("models") or {}) if isinstance(cfg, dict) else {}
    openrouter_cfg = (models_cfg.get("openrouter") or {}) if isinstance(models_cfg, dict) else {}
    if isinstance(openrouter_cfg, dict) and openrouter_cfg.get("enabled", False):
        url_models = str(openrouter_cfg.get("url_models", "https://openrouter.ai/rankings"))
        url_apps = str(openrouter_cfg.get("url_apps", "https://openrouter.ai/rankings/apps"))
        include_apps = bool(openrouter_cfg.get("include_apps", False))

        top_models, top_apps = fetch_openrouter_rankings(
            url_models=url_models,
            url_apps=url_apps,
            include_apps=include_apps,
            top_n=top_n,
        )

        if top_models:
            lines.append("### OpenRouterï¼šæ¨¡å‹çƒ­åº¦æ¦œï¼ˆrequestsï¼‰")
            lines.append("")
            lines.append("> å£å¾„ï¼šä» OpenRouter æ’è¡Œé¡µæ³¨å…¥çš„æ¨¡å‹ç»Ÿè®¡ä¸­æå– request_countï¼ˆè¯·æ±‚é‡ï¼‰ã€‚")
            lines.append("")
            lines.extend(_render_ranked_list(top_models))
            lines.append("")

        if include_apps and top_apps:
            lines.append("### OpenRouterï¼šTop Appsï¼ˆopt-in ä½¿ç”¨è¿½è¸ªï¼‰")
            lines.append("")
            lines.append("> å£å¾„ï¼šOpenRouter å…¬å¸ƒçš„ opt-in åº”ç”¨ total_tokensï¼ˆä¸æ˜¯å…¨ç½‘ï¼›ä»…ç»Ÿè®¡é€‰æ‹©ä¸ŠæŠ¥çš„åº”ç”¨ï¼‰ã€‚")
            lines.append("")
            lines.extend(_render_ranked_list(top_apps))
            lines.append("")

    # ===== æ¨¡å‹æ¦œï¼šLMArenaï¼ˆArena-Hard-Autoï¼‰ =====
    lmarena_cfg = (models_cfg.get("lmarena") or {}) if isinstance(models_cfg, dict) else {}
    if isinstance(lmarena_cfg, dict) and lmarena_cfg.get("enabled", False):
        csv_url = str(lmarena_cfg.get("arena_hard_auto_csv", "") or "").strip()
        if csv_url:
            items, date_str = fetch_lmarena_arena_hard_auto(csv_url=csv_url, top_n=top_n)
            if items:
                lines.append("### LMArenaï¼šArena-Hard-Autoï¼ˆèƒ½åŠ›æ¦œï¼‰")
                lines.append("")
                # é¿å…è¯¯è¯»ï¼šæ˜ç¡®ä¸æ˜¯ Eloï¼ˆäººç±»æŠ•ç¥¨ï¼‰
                if date_str:
                    lines.append(f"> å£å¾„ï¼šArena-Hard-Auto åˆ†æ•°ï¼ˆdate={date_str}ï¼‰ã€‚æ³¨æ„ï¼šè¿™ä¸æ˜¯äººç±»æŠ•ç¥¨ Eloï¼Œè€Œæ˜¯ LMArena å‘å¸ƒçš„åŸºå‡†è¯„åˆ†ã€‚")
                else:
                    lines.append("> å£å¾„ï¼šArena-Hard-Auto åˆ†æ•°ã€‚æ³¨æ„ï¼šè¿™ä¸æ˜¯äººç±»æŠ•ç¥¨ Eloï¼Œè€Œæ˜¯ LMArena å‘å¸ƒçš„åŸºå‡†è¯„åˆ†ã€‚")
                lines.append("")
                lines.extend(_render_ranked_list(items))
                lines.append("")

    # ===== å·¥å…·æ¦œï¼šPyPI / npm =====
    tools_cfg = (cfg.get("tools") or {}) if isinstance(cfg, dict) else {}

    # PyPI
    pypi_cfg = (tools_cfg.get("pypi") or {}) if isinstance(tools_cfg, dict) else {}
    if isinstance(pypi_cfg, dict) and pypi_cfg.get("enabled", False):
        pkgs = pypi_cfg.get("packages") or []
        results: List[Tuple[str, str, int]] = []  # (name, pkg, downloads)
        failed: List[str] = []
        if isinstance(pkgs, list):
            for item in pkgs:
                if not isinstance(item, dict):
                    continue
                pkg = str(item.get("package", "") or "").strip()
                name = str(item.get("name", "") or pkg).strip()
                if not pkg:
                    continue
                downloads = fetch_pypi_downloads_last_week(pkg)
                if downloads is None:
                    failed.append(pkg)
                    continue
                results.append((name, pkg, downloads))
        if results:
            results.sort(key=lambda x: x[2], reverse=True)
            lines.append("### å·¥å…·ï¼šPyPI ä¸‹è½½é‡ï¼ˆlast_weekï¼‰")
            lines.append("")
            lines.append("> å£å¾„ï¼špypistats æœ€è¿‘ä¸€å‘¨ä¸‹è½½é‡ï¼ˆlast_weekï¼‰ã€‚")
            if failed:
                lines.append(f"> è·å–å¤±è´¥å·²è·³è¿‡ï¼š{', '.join(failed)}")
            lines.append("")
            for idx, (name, pkg, downloads) in enumerate(results[:top_n], 1):
                lines.append(f"{idx}. **{name}** (`{pkg}`) â€” {downloads:,} downloads")
            lines.append("")

    # npm
    npm_cfg = (tools_cfg.get("npm") or {}) if isinstance(tools_cfg, dict) else {}
    if isinstance(npm_cfg, dict) and npm_cfg.get("enabled", False):
        pkgs = npm_cfg.get("packages") or []
        results = []  # (name, pkg, downloads)
        failed: List[str] = []
        if isinstance(pkgs, list):
            for item in pkgs:
                if not isinstance(item, dict):
                    continue
                pkg = str(item.get("package", "") or "").strip()
                name = str(item.get("name", "") or pkg).strip()
                if not pkg:
                    continue
                downloads = fetch_npm_downloads_last_week(pkg)
                if downloads is None:
                    failed.append(pkg)
                    continue
                results.append((name, pkg, downloads))
        if results:
            results.sort(key=lambda x: x[2], reverse=True)
            lines.append("### å·¥å…·ï¼šnpm ä¸‹è½½é‡ï¼ˆlast-weekï¼‰")
            lines.append("")
            lines.append("> å£å¾„ï¼šnpm downloads API `last-week` ä¸‹è½½é‡ã€‚")
            if failed:
                lines.append(f"> è·å–å¤±è´¥å·²è·³è¿‡ï¼š{', '.join(failed)}")
            lines.append("")
            for idx, (name, pkg, downloads) in enumerate(results[:top_n], 1):
                lines.append(f"{idx}. **{name}** (`{pkg}`) â€” {downloads:,} downloads")
            lines.append("")

    # å¦‚æœæ²¡æœ‰ä»»ä½•åˆ†æ¦œæˆåŠŸï¼Œè¿”å›ç©ºå­—ç¬¦ä¸²ï¼ˆä¸æ±¡æŸ“å‘¨æŠ¥ï¼‰
    has_payload = any(line.startswith("### ") for line in lines)
    if not has_payload:
        return ""
    return "\n".join(lines).strip()

