# coding=utf-8
"""
è®ºæ–‡ä¸“åŒºï¼šæ–°å¢è®ºæ–‡ç›‘æ§ä¸å•ç¯‡è§£è¯»æŠ¥å‘Šç”Ÿæˆï¼ˆç”¨äº GitHub Pages é“¾æ¥ï¼‰

è®¾è®¡ç›®æ ‡ï¼š
- åœ¨å¢é‡è¿è¡Œæ—¶ï¼Œä»æŒ‡å®š arXiv RSS feeds ä¸­æ‰¾å‡ºæ–°å¢è®ºæ–‡
- é€šè¿‡ LLMï¼ˆgemini-3-pro-previewï¼‰ç­›é€‰â€œé«˜ä»·å€¼â€è®ºæ–‡å¹¶ç”Ÿæˆå¯è¯»æ€§é«˜çš„è§£è¯»æŠ¥å‘Š
- æŠ¥å‘Šè½ç›˜åˆ° site/ ç›®å½•ï¼Œä¾› GitHub Pages å‘å¸ƒä¸ºå›ºå®šé“¾æ¥

çº¦æŸä¸é˜²å¹»è§‰ï¼š
- ä»…åŸºäº RSS æ¡ç›®ï¼ˆæ ‡é¢˜/æ‘˜è¦/é“¾æ¥ï¼‰åˆ†æï¼Œç¦æ­¢è‡†é€ å®éªŒç»†èŠ‚
"""

from __future__ import annotations

import html as _html
import json
import os
import re
from dataclasses import dataclass
from datetime import datetime
from pathlib import Path
from typing import Any, Dict, List, Optional, Tuple

import requests


_ARXIV_ABS_RE = re.compile(r"arxiv\.org/abs/([^?#]+)", re.IGNORECASE)
_ARXIV_PDF_RE = re.compile(r"arxiv\.org/pdf/([^?#]+?)(?:\.pdf)?$", re.IGNORECASE)
_AR5IV_HTML_RE = re.compile(r"ar5iv\.labs\.arxiv\.org/html/([^?#]+)$", re.IGNORECASE)


def _safe_json_extract(text: str) -> Optional[str]:
    if not text:
        return None
    s = text.strip()
    if s.startswith("```"):
        s = s.strip("`").strip()
        lines = s.splitlines()
        if lines and lines[0].strip().lower() in ("json", "javascript"):
            s = "\n".join(lines[1:]).strip()
    l = s.find("[")
    r = s.rfind("]")
    if l != -1 and r != -1 and r > l:
        return s[l : r + 1].strip()
    l = s.find("{")
    r = s.rfind("}")
    if l != -1 and r != -1 and r > l:
        return s[l : r + 1].strip()
    return None


def _load_prompt_template(prompt_path: Path) -> Tuple[str, str]:
    """
    å…¼å®¹ TrendRadar ç°æœ‰ prompt æ ¼å¼ï¼š
    - å« [system] ä¸ [user] åˆ†æ®µ
    - æˆ–åªæœ‰çº¯æ–‡æœ¬ï¼ˆè§†ä¸º userï¼‰
    """
    content = prompt_path.read_text(encoding="utf-8")
    if "[system]" in content and "[user]" in content:
        parts = content.split("[user]", 1)
        system_part = parts[0]
        user_part = parts[1] if len(parts) > 1 else ""
        system_prompt = system_part.split("[system]", 1)[1].strip() if "[system]" in system_part else ""
        user_prompt = user_part.strip()
        return system_prompt, user_prompt
    return "", content.strip()


def _guess_pages_base_url(config_base: str = "") -> str:
    """
    æ¨å¯¼ GitHub Pages base urlï¼š
    - ä¼˜å…ˆä½¿ç”¨é…ç½®/ç¯å¢ƒå˜é‡æä¾›çš„å›ºå®š base url
    - å¦åˆ™æ ¹æ® GITHUB_REPOSITORY æ¨å¯¼ä¸º https://{owner}.github.io/{repo}/
    """
    if config_base:
        base = config_base.strip()
        if base and not base.endswith("/"):
            base += "/"
        return base

    repo = (os.environ.get("GITHUB_REPOSITORY", "") or "").strip()
    if not repo or "/" not in repo:
        return ""
    owner, name = repo.split("/", 1)
    return f"https://{owner}.github.io/{name}/"


def _extract_arxiv_id(url: str) -> str:
    if not url:
        return ""
    m = _ARXIV_ABS_RE.search(url)
    if m:
        return m.group(1).strip()
    # å…œåº•ï¼šå–æœ€åè·¯å¾„æ®µ
    try:
        from urllib.parse import urlparse

        path = urlparse(url).path or ""
        parts = [p for p in path.split("/") if p]
        return parts[-1] if parts else ""
    except Exception:
        return ""


def _normalize_arxiv_id(arxiv_id: str) -> str:
    s = (arxiv_id or "").strip()
    if not s:
        return ""
    s = s.replace(".pdf", "")
    s = s.strip("/")
    return s


def _arxiv_abs_url(arxiv_id: str) -> str:
    aid = _normalize_arxiv_id(arxiv_id)
    return f"https://arxiv.org/abs/{aid}" if aid else ""


def _arxiv_pdf_url(arxiv_id: str) -> str:
    aid = _normalize_arxiv_id(arxiv_id)
    return f"https://arxiv.org/pdf/{aid}.pdf" if aid else ""


def _ar5iv_url(arxiv_id: str) -> str:
    aid = _normalize_arxiv_id(arxiv_id)
    return f"https://ar5iv.labs.arxiv.org/html/{aid}" if aid else ""


def _clip_text(text: str, max_chars: int) -> str:
    if not text:
        return ""
    if not max_chars or max_chars <= 0:
        return text
    if len(text) <= max_chars:
        return text
    # ä¿ç•™å¤´å°¾ï¼Œå°½é‡è¦†ç›–ä¿¡æ¯å¯†åº¦ï¼ˆé˜²æ­¢åªæˆªåˆ°å‚è€ƒæ–‡çŒ®ï¼‰
    head = text[: int(max_chars * 0.7)]
    tail = text[-int(max_chars * 0.3) :]
    return head + "\n\n...[TRUNCATED]...\n\n" + tail


def _fetch_text_from_ar5iv(arxiv_id: str, timeout: int) -> Optional[str]:
    url = _ar5iv_url(arxiv_id)
    if not url:
        return None
    resp = requests.get(url, timeout=timeout, headers={"User-Agent": "TrendRadar/2.0"})
    if resp.status_code != 200:
        return None
    html_text = resp.text or ""
    if not html_text.strip():
        return None
    # ç²—ç•¥å»æ ‡ç­¾ï¼šä¿ç•™æ®µè½æ–‡æœ¬
    text = re.sub(r"(?is)<(script|style).*?>.*?</\\1>", " ", html_text)
    text = re.sub(r"(?is)<[^>]+>", " ", text)
    # ä»…æŠ˜å â€œç©ºç™½å­—ç¬¦â€ï¼Œä¸è¦è¯¯åˆ å­—æ¯ï¼ˆé¿å…é”™è¯¯çš„è½¬ä¹‰å¯¼è‡´ t/x/b/c/r è¢«å½“ä½œå¯æ›¿æ¢å­—ç¬¦ï¼‰
    text = re.sub(r"[ \t\x0b\x0c\r]+", " ", text)
    text = re.sub(r"\\n{3,}", "\\n\\n", text)
    text = text.strip()
    # æç«¯çŸ­åˆ™è§†ä¸ºå¤±è´¥
    return text if len(text) >= 2000 else None


def _fetch_text_from_arxiv_abs(arxiv_id: str, timeout: int) -> Optional[str]:
    url = _arxiv_abs_url(arxiv_id)
    if not url:
        return None
    resp = requests.get(url, timeout=timeout, headers={"User-Agent": "TrendRadar/2.0"})
    if resp.status_code != 200:
        return None
    html_text = resp.text or ""
    if not html_text.strip():
        return None
    # æå– abstractï¼ˆarXiv é¡µé¢æœ‰ <blockquote class="abstract">ï¼‰
    m = re.search(r'(?is)<blockquote[^>]*class="abstract[^"]*"[^>]*>(.*?)</blockquote>', html_text)
    if not m:
        return None
    block = m.group(1)
    block = re.sub(r"(?is)<[^>]+>", " ", block)
    block = re.sub(r"[ \t\x0b\x0c\r]+", " ", block).strip()
    # ä¿®å¤ arXiv é¡µé¢å¶å‘çš„â€œé€å­—ç¬¦åˆ†éš”â€é—®é¢˜ï¼šæŠŠå•å­—æ¯ token é‡æ–°æ‹¼å›å•è¯
    # ç¤ºä¾‹ï¼šA b s t r a c t : T h i s ... -> Abstract: This ...
    block = re.sub(r"(?<=\\b[A-Za-z])\\s+(?=[A-Za-z]\\b)", "", block)
    return block if len(block) >= 200 else None


def _fetch_text_from_arxiv_pdf(arxiv_id: str, timeout: int) -> Optional[str]:
    url = _arxiv_pdf_url(arxiv_id)
    if not url:
        return None
    resp = requests.get(url, timeout=timeout, headers={"User-Agent": "TrendRadar/2.0"})
    if resp.status_code != 200 or not resp.content:
        return None
    try:
        from pypdf import PdfReader
    except Exception:
        return None

    try:
        import io

        reader = PdfReader(io.BytesIO(resp.content))
        texts = []
        # åªæŠ½å–å‰è‹¥å¹²é¡µä»¥æ§åˆ¶æˆæœ¬ä¸å™ªå£°ï¼ˆå…¨æ–‡å¤ªé•¿ï¼‰
        max_pages = min(len(reader.pages), 12)
        for i in range(max_pages):
            try:
                page_text = reader.pages[i].extract_text() or ""
                if page_text.strip():
                    texts.append(page_text)
            except Exception:
                continue
        joined = "\n\n".join(texts).strip()
        return joined if len(joined) >= 1500 else None
    except Exception:
        return None


def fetch_paper_content(
    candidate: "PaperCandidate",
    paper_zone_cfg: Dict[str, Any],
) -> Tuple[str, str]:
    """
    è·å–è®ºæ–‡â€œå°½å¯èƒ½æ¥è¿‘åŸæ–‡â€çš„æ–‡æœ¬å†…å®¹ã€‚
    è¿”å›ï¼š(content_source, paper_content)
    """
    priority = paper_zone_cfg.get("CONTENT_SOURCE_PRIORITY") or ["ar5iv", "pdf", "abs", "rss"]
    timeout = int(paper_zone_cfg.get("CONTENT_FETCH_TIMEOUT") or 20)
    max_chars = int(paper_zone_cfg.get("MAX_CONTENT_CHARS") or 45000)

    arxiv_id = candidate.arxiv_id or _extract_arxiv_id(candidate.url)
    arxiv_id = _normalize_arxiv_id(arxiv_id)

    # fallbackï¼šrss summary
    rss_fallback = candidate.summary or ""

    for src in priority:
        s = (src or "").strip().lower()
        try:
            if s == "ar5iv" and arxiv_id:
                text = _fetch_text_from_ar5iv(arxiv_id, timeout)
                if text:
                    return "ar5iv", _clip_text(text, max_chars)
            if s == "pdf" and arxiv_id:
                text = _fetch_text_from_arxiv_pdf(arxiv_id, timeout)
                if text:
                    return "pdf", _clip_text(text, max_chars)
            if s == "abs" and arxiv_id:
                text = _fetch_text_from_arxiv_abs(arxiv_id, timeout)
                if text:
                    return "abs", _clip_text(text, max_chars)
            if s == "rss":
                if rss_fallback.strip():
                    return "rss", _clip_text(rss_fallback, max_chars)
        except Exception:
            continue

    # å…œåº•
    return ("rss" if rss_fallback.strip() else "unknown"), _clip_text(rss_fallback, max_chars)


def _slugify(s: str) -> str:
    s = (s or "").strip()
    if not s:
        return ""
    s = s.replace("/", "_")
    s = re.sub(r"[^A-Za-z0-9._-]+", "_", s)
    return s.strip("_").lower()


def _md_inline_format(text: str) -> str:
    # ç®€å•å¤„ç†ï¼šé“¾æ¥ä¸ç²—ä½“
    t = _html.escape(text, quote=False)
    # **bold**
    t = re.sub(r"\*\*([^*]+)\*\*", r"<strong>\1</strong>", t)
    # [text](url)
    t = re.sub(r"\[([^\]]+)\]\((https?://[^)]+)\)", r"<a href=\"\2\" target=\"_blank\" rel=\"noopener noreferrer\">\1</a>", t)
    return t


def _markdown_to_html(md: str) -> str:
    """
    è½»é‡ markdown æ¸²æŸ“å™¨ï¼ˆè¦†ç›–æœ¬é¡¹ç›®è¾“å‡ºç»“æ„ï¼šæ ‡é¢˜/åˆ—è¡¨/æ®µè½/ä»£ç å—/é“¾æ¥/ç²—ä½“ï¼‰ã€‚
    ç›®æ ‡ï¼šä¸å¼•å…¥é¢å¤–ä¾èµ–ï¼Œä¹Ÿèƒ½åœ¨ Pages ä¸Šå¯è¯»ã€‚
    """
    if md is None:
        return ""
    lines = md.splitlines()
    out: List[str] = []
    in_code = False
    in_ul = False

    def close_ul():
        nonlocal in_ul
        if in_ul:
            out.append("</ul>")
            in_ul = False

    for raw in lines:
        line = raw.rstrip("\n")
        if line.strip().startswith("```"):
            close_ul()
            if not in_code:
                in_code = True
                out.append("<pre><code>")
            else:
                in_code = False
                out.append("</code></pre>")
            continue

        if in_code:
            out.append(_html.escape(line))
            continue

        if not line.strip():
            close_ul()
            out.append("")
            continue

        # headings
        if line.startswith("### "):
            close_ul()
            out.append(f"<h3>{_md_inline_format(line[4:].strip())}</h3>")
            continue
        if line.startswith("## "):
            close_ul()
            out.append(f"<h2>{_md_inline_format(line[3:].strip())}</h2>")
            continue
        if line.startswith("# "):
            close_ul()
            out.append(f"<h1>{_md_inline_format(line[2:].strip())}</h1>")
            continue

        # unordered list
        if line.lstrip().startswith("- "):
            if not in_ul:
                out.append("<ul>")
                in_ul = True
            item = line.lstrip()[2:].strip()
            out.append(f"<li>{_md_inline_format(item)}</li>")
            continue

        # paragraph
        close_ul()
        out.append(f"<p>{_md_inline_format(line.strip())}</p>")

    close_ul()
    # æ¸…ç†è¿ç»­ç©ºè¡Œ
    html_text = "\n".join(out)
    html_text = re.sub(r"\n{3,}", "\n\n", html_text)
    return html_text


@dataclass
class PaperCandidate:
    id: str
    feed_id: str
    feed_name: str
    title: str
    url: str
    published_at: str
    summary: str

    arxiv_id: str = ""
    slug: str = ""


@dataclass
class PaperDecision:
    id: str
    score: int
    keep: bool
    reason: str


def collect_paper_candidates(
    raw_rss_items: Optional[List[Dict[str, Any]]],
    feed_ids: List[str],
) -> List[PaperCandidate]:
    if not raw_rss_items or not feed_ids:
        return []
    feed_id_set = {x.strip() for x in feed_ids if x and str(x).strip()}
    candidates: List[PaperCandidate] = []
    for item in raw_rss_items:
        fid = (item.get("feed_id") or "").strip()
        if fid not in feed_id_set:
            continue
        title = (item.get("title") or "").strip()
        url = (item.get("url") or "").strip()
        if not title or not url:
            continue
        published_at = (item.get("published_at") or "").strip()
        summary = (item.get("summary") or "").strip()
        feed_name = (item.get("feed_name") or fid).strip()
        pid = _extract_arxiv_id(url) or url
        slug = _slugify(pid) or _slugify(title[:80])
        candidates.append(
            PaperCandidate(
                id=pid,
                feed_id=fid,
                feed_name=feed_name,
                title=title,
                url=url,
                published_at=published_at,
                summary=summary,
                arxiv_id=_extract_arxiv_id(url),
                slug=slug,
            )
        )
    return candidates


def decide_high_value_papers(
    candidates: List[PaperCandidate],
    ai_config: Dict[str, Any],
    model: str,
    reasoning_effort: str,
    min_score: int,
    max_reports_per_run: int,
    timeout: int = 90,
) -> List[PaperDecision]:
    """
    ç”¨ LLM å¯¹å€™é€‰è®ºæ–‡åšâ€œé«˜ä»·å€¼â€åˆ¤å®šä¸æ‰“åˆ†ï¼Œå¹¶è¿”å› top Kã€‚
    """
    if not candidates:
        return []

    try:
        from trendradar.ai.client import AIClient
    except Exception as e:
        raise RuntimeError(f"è®ºæ–‡è§£è¯»ä¾èµ–ä¸å¯ç”¨: {type(e).__name__}: {e}") from e

    cfg = dict(ai_config or {})
    cfg["MODEL"] = model
    # è¿™é‡Œä¸å¼ºè¡Œè¦†ç›– MAX_TOKENSï¼šç”± AIClient é»˜è®¤é…ç½®æ§åˆ¶ï¼›åªåœ¨ chat è°ƒç”¨æ—¶é™åˆ¶
    client = AIClient(cfg)

    payload = []
    for c in candidates:
        payload.append(
            {
                "id": c.id,
                "feed_id": c.feed_id,
                "title": c.title,
                "url": c.url,
                "published_at": c.published_at,
                "summary": (c.summary[:1200] if c.summary else ""),
            }
        )

    system = (
        "ä½ æ˜¯ä¸€å AI ç ”ç©¶ä¸äº§å“è¯„å®¡å®˜ã€‚ä½ è¦ä»â€œæ–°è®ºæ–‡æ ‡é¢˜+æ‘˜è¦â€ä¸­è¯„ä¼°å…¶å¯¹ AI äº§å“ç»ç†çš„ä»·å€¼ã€‚\n"
        "é«˜ä»·å€¼çš„åˆ¤å®šæ ‡å‡†ï¼ˆä¼˜å…ˆçº§ä»é«˜åˆ°ä½ï¼‰ï¼š\n"
        "1) ç›´æ¥æ¨åŠ¨æ¨¡å‹/Agent/å¤šæ¨¡æ€èƒ½åŠ›è¾¹ç•Œæˆ–è¯„æµ‹æ–¹æ³•\n"
        "2) æ˜ç¡®å¯è½åœ°çš„å·¥ç¨‹æ–¹æ³•ï¼ˆæ•ˆç‡ã€å¯é æ€§ã€å®‰å…¨ã€å¯æ§æ€§ï¼‰\n"
        "3) ä¸ç«¯ä¾§/å¯ç©¿æˆ´/XR/HCI äº¤äº’å¼ºç›¸å…³ï¼ˆéšç§ã€ä½åŠŸè€—ã€å®æ—¶ï¼‰\n\n"
        "å¿…é¡»é¿å…ï¼šä»…å‡­æ ‡é¢˜çŒœæµ‹ç»†èŠ‚ï¼›å¦‚æœæ‘˜è¦ä¸æ”¯æŒç»“è®ºï¼Œè¦é™ä½è¯„åˆ†å¹¶è¯´æ˜ã€‚\n"
        "è¾“å‡ºå¿…é¡»æ˜¯ JSON æ•°ç»„ï¼ˆä¸è¦è¾“å‡ºè§£é‡Šï¼‰ã€‚"
    )
    user = (
        "è¯·å¯¹æ¯æ¡è®ºæ–‡è¾“å‡ºä¸€ä¸ªå¯¹è±¡ï¼š\n"
        '- id: ä¸è¾“å…¥ä¸€è‡´\n'
        '- keep: true/falseï¼ˆæ˜¯å¦å€¼å¾—ç”Ÿæˆâ€œå•ç¯‡è§£è¯»æŠ¥å‘Šâ€ï¼‰\n'
        '- score: 0-100ï¼ˆä»·å€¼è¯„åˆ†ï¼‰\n'
        '- reason: ç®€çŸ­åŸå› ï¼ˆ<=20å­—ï¼‰\n\n'
        f"è¾“å…¥ï¼š{json.dumps(payload, ensure_ascii=False)}"
    )

    raw = client.chat(
        messages=[
            {"role": "system", "content": system},
            {"role": "user", "content": user},
        ],
        temperature=0.0,
        timeout=timeout,
        max_tokens=1400,
        reasoning_effort=reasoning_effort,
    )

    extracted = _safe_json_extract(raw)
    if not extracted:
        raise ValueError("é«˜ä»·å€¼åˆ¤å®šè¾“å‡ºæœªæ‰¾åˆ°å¯è§£æ JSON")
    data = json.loads(extracted)
    if not isinstance(data, list):
        raise ValueError("é«˜ä»·å€¼åˆ¤å®šè¾“å‡ºä¸æ˜¯ JSON æ•°ç»„")

    decisions: List[PaperDecision] = []
    for row in data:
        if not isinstance(row, dict):
            continue
        rid = str(row.get("id", "")).strip()
        if not rid:
            continue
        score = int(row.get("score", 0) or 0)
        keep = bool(row.get("keep", False))
        reason = str(row.get("reason", "")).strip()
        decisions.append(PaperDecision(id=rid, score=score, keep=keep, reason=reason))

    # è¿‡æ»¤ä¸æ’åº
    picked = [d for d in decisions if d.keep and d.score >= int(min_score or 0)]
    picked.sort(key=lambda x: (-x.score, x.id))

    if max_reports_per_run and max_reports_per_run > 0:
        picked = picked[:max_reports_per_run]
    return picked


def generate_single_paper_report(
    candidate: PaperCandidate,
    ai_config: Dict[str, Any],
    paper_zone_cfg: Dict[str, Any],
    now: datetime,
    project_root: str,
) -> Tuple[str, str]:
    """
    ç”Ÿæˆå•ç¯‡æŠ¥å‘Šï¼šè¿”å› (markdown, html)
    """
    try:
        from trendradar.ai.client import AIClient
    except Exception as e:
        raise RuntimeError(f"è®ºæ–‡è§£è¯»ä¾èµ–ä¸å¯ç”¨: {type(e).__name__}: {e}") from e

    model = (paper_zone_cfg.get("MODEL") or "gemini-3-pro-preview").strip()
    reasoning_effort = (paper_zone_cfg.get("REASONING_EFFORT") or "high").strip()

    cfg = dict(ai_config or {})
    cfg["MODEL"] = model
    client = AIClient(cfg)

    config_dir = Path(project_root) / "config"
    prompt_file = (paper_zone_cfg.get("PROMPT_FILE") or "paper_analysis_prompt.txt").strip()
    prompt_path = config_dir / prompt_file
    if not prompt_path.exists():
        raise FileNotFoundError(f"è®ºæ–‡è§£è¯»æç¤ºè¯ä¸å­˜åœ¨: {prompt_path}")

    system_prompt, user_template = _load_prompt_template(prompt_path)

    user_prompt = user_template
    content_source, paper_content = fetch_paper_content(candidate, paper_zone_cfg)
    user_prompt = user_prompt.replace("{paper_title}", candidate.title)
    user_prompt = user_prompt.replace("{paper_url}", candidate.url)
    user_prompt = user_prompt.replace("{content_source}", content_source)
    user_prompt = user_prompt.replace("{paper_content}", paper_content)

    messages = []
    if system_prompt:
        messages.append({"role": "system", "content": system_prompt})
    messages.append({"role": "user", "content": user_prompt})

    md = client.chat(
        messages=messages,
        temperature=0.2,
        timeout=paper_zone_cfg.get("TIMEOUT", 180) or 180,
        max_tokens=paper_zone_cfg.get("MAX_TOKENS", 8000) or 8000,
        reasoning_effort=reasoning_effort,
    ).strip()

    body_html = _markdown_to_html(md)
    title_esc = _html.escape(candidate.title)
    paper_url_esc = _html.escape(candidate.url, quote=True)

    full_html = f"""<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>{title_esc} - TrendRadar Paper Brief</title>
  <style>
    :root {{
      color-scheme: light dark;
    }}
    body {{
      font-family: -apple-system, BlinkMacSystemFont, Segoe UI, Roboto, Helvetica, Arial, \"PingFang SC\", \"Hiragino Sans GB\", \"Microsoft YaHei\", sans-serif;
      line-height: 1.6;
      margin: 0;
      padding: 0;
    }}
    header {{
      padding: 24px 16px;
      border-bottom: 1px solid rgba(127,127,127,0.25);
    }}
    main {{
      max-width: 920px;
      margin: 0 auto;
      padding: 20px 16px 56px;
    }}
    a {{ color: inherit; }}
    code, pre {{
      font-family: ui-monospace, SFMono-Regular, Menlo, Monaco, Consolas, \"Liberation Mono\", \"Courier New\", monospace;
    }}
    pre {{
      padding: 12px;
      border-radius: 8px;
      overflow: auto;
      background: rgba(127,127,127,0.12);
    }}
    h1, h2, h3 {{ line-height: 1.25; }}
    .meta {{
      color: rgba(127,127,127,0.9);
      font-size: 14px;
    }}
    .badge {{
      display: inline-block;
      padding: 2px 8px;
      border-radius: 999px;
      border: 1px solid rgba(127,127,127,0.35);
      font-size: 12px;
      margin-right: 8px;
    }}
  </style>
</head>
<body>
  <header>
    <div class="meta">
      <span class="badge">TrendRadar</span>
      <span class="badge">Paper Brief</span>
      <span class="badge">{_html.escape(candidate.feed_name)}</span>
      <span>ç”Ÿæˆæ—¶é—´ï¼š{_html.escape(now.strftime("%Y-%m-%d %H:%M:%S"))}</span>
    </div>
    <h1>{title_esc}</h1>
    <div class="meta">
      <a href="{paper_url_esc}" target="_blank" rel="noopener noreferrer">åŸæ–‡é“¾æ¥</a>
      {" Â· å‘å¸ƒæ—¶é—´ï¼š" + _html.escape(candidate.published_at) if candidate.published_at else ""}
    </div>
  </header>
  <main>
    {body_html}
  </main>
</body>
</html>
"""
    return md, full_html


def write_paper_pages(
    candidates: List[PaperCandidate],
    decisions: List[PaperDecision],
    ai_config: Dict[str, Any],
    paper_zone_cfg: Dict[str, Any],
    now: datetime,
    project_root: str,
) -> List[Dict[str, Any]]:
    """
    ç”Ÿæˆå¹¶å†™å…¥é¡µé¢ï¼Œè¿”å›ç”¨äºæ¨é€çš„ paper_reports åˆ—è¡¨ã€‚
    """
    output_dir = (paper_zone_cfg.get("OUTPUT_DIR") or "site").strip()
    site_dir = Path(project_root) / output_dir
    papers_dir = site_dir / "papers"
    papers_dir.mkdir(parents=True, exist_ok=True)

    base_url = _guess_pages_base_url(paper_zone_cfg.get("PAGES_BASE_URL", ""))

    # å»ºç«‹ id -> candidate
    cand_map = {c.id: c for c in candidates}

    reports: List[Dict[str, Any]] = []
    for d in decisions:
        c = cand_map.get(d.id)
        if not c:
            continue
        slug = c.slug or _slugify(c.id)
        if not slug:
            continue

        out_path = papers_dir / slug / "index.html"
        out_path.parent.mkdir(parents=True, exist_ok=True)

        # è‹¥å·²å­˜åœ¨åˆ™ä¸é‡å¤ç”Ÿæˆï¼ˆé¿å…é‡å¤æ¶ˆè€—ï¼‰
        if not out_path.exists():
            md, html_text = generate_single_paper_report(
                candidate=c,
                ai_config=ai_config,
                paper_zone_cfg=paper_zone_cfg,
                now=now,
                project_root=project_root,
            )
            out_path.write_text(html_text, encoding="utf-8")
            # æ—è·¯ä¿å­˜ markdownï¼ˆæ–¹ä¾¿ diff/å¤ç”¨ï¼‰
            (out_path.parent / "report.md").write_text(md, encoding="utf-8")

        report_url = f"{base_url}papers/{slug}/" if base_url else f"papers/{slug}/"
        reports.append(
            {
                "title": c.title,
                "paper_url": c.url,
                "report_url": report_url,
                "feed_name": c.feed_name,
                "feed_id": c.feed_id,
                "published_at": c.published_at,
                "score": d.score,
                "reason": d.reason,
                "slug": slug,
                "local_path": str(out_path),
            }
        )

    # ç”Ÿæˆç´¢å¼•é¡µï¼ˆç®€æ´å¯è¯»ï¼‰
    _write_papers_index(papers_dir, reports, now, base_url)
    _write_root_index(site_dir, now, base_url)

    return reports


def _write_papers_index(papers_dir: Path, reports: List[Dict[str, Any]], now: datetime, base_url: str) -> None:
    # æŒ‰ score / æ—¶é—´æ’åºï¼ˆæ—¶é—´å¯èƒ½ç¼ºå¤±ï¼Œå…ˆç”¨ scoreï¼‰
    sorted_reports = sorted(reports, key=lambda x: (-int(x.get("score", 0) or 0), x.get("published_at", "")),)
    items = []
    for r in sorted_reports[:200]:
        title = _html.escape(r.get("title", ""))
        url = _html.escape(r.get("report_url", ""), quote=True)
        score = int(r.get("score", 0) or 0)
        feed = _html.escape(r.get("feed_name", ""))
        items.append(f"<li><a href=\"{url}\">{title}</a> <span class=\"meta\">({feed} Â· score {score})</span></li>")

    html_text = f"""<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>TrendRadar è®ºæ–‡ä¸“åŒº</title>
  <style>
    :root {{ color-scheme: light dark; }}
    body {{ font-family: -apple-system, BlinkMacSystemFont, Segoe UI, Roboto, Helvetica, Arial, \"PingFang SC\", \"Hiragino Sans GB\", \"Microsoft YaHei\", sans-serif; line-height: 1.6; }}
    main {{ max-width: 920px; margin: 0 auto; padding: 24px 16px 56px; }}
    .meta {{ color: rgba(127,127,127,0.9); font-size: 13px; }}
    a {{ color: inherit; }}
  </style>
</head>
<body>
  <main>
    <h1>ğŸ“š TrendRadar è®ºæ–‡ä¸“åŒº</h1>
    <div class="meta">æ›´æ–°æ—¶é—´ï¼š{_html.escape(now.strftime("%Y-%m-%d %H:%M:%S"))}</div>
    <p class="meta">è¯´æ˜ï¼šæ­¤é¡µé¢åˆ—å‡ºè¿‘æœŸç”± TrendRadar è‡ªåŠ¨ç”Ÿæˆçš„è®ºæ–‡è§£è¯»æŠ¥å‘Šï¼ˆå•ç¯‡ï¼‰ã€‚</p>
    <ul>
      {''.join(items) if items else '<li class="meta">æš‚æ— æŠ¥å‘Š</li>'}
    </ul>
    <hr/>
    <div class="meta">Base URLï¼š{_html.escape(base_url) if base_url else 'æœªé…ç½®ï¼ˆå°†ä»¥ç›¸å¯¹é“¾æ¥å±•ç¤ºï¼‰'}</div>
  </main>
</body>
</html>
"""
    (papers_dir / "index.html").write_text(html_text, encoding="utf-8")


def _write_root_index(site_dir: Path, now: datetime, base_url: str) -> None:
    site_dir.mkdir(parents=True, exist_ok=True)
    papers_url = f"{base_url}papers/" if base_url else "papers/"
    html_text = f"""<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>TrendRadar Reports</title>
  <style>
    :root {{ color-scheme: light dark; }}
    body {{ font-family: -apple-system, BlinkMacSystemFont, Segoe UI, Roboto, Helvetica, Arial, \"PingFang SC\", \"Hiragino Sans GB\", \"Microsoft YaHei\", sans-serif; line-height: 1.6; }}
    main {{ max-width: 920px; margin: 0 auto; padding: 24px 16px 56px; }}
    .meta {{ color: rgba(127,127,127,0.9); font-size: 13px; }}
    a {{ color: inherit; }}
  </style>
</head>
<body>
  <main>
    <h1>TrendRadar Reports</h1>
    <div class="meta">æ›´æ–°æ—¶é—´ï¼š{_html.escape(now.strftime("%Y-%m-%d %H:%M:%S"))}</div>
    <ul>
      <li><a href=\"{_html.escape(papers_url, quote=True)}\">è®ºæ–‡ä¸“åŒºï¼ˆPapersï¼‰</a></li>
    </ul>
  </main>
</body>
</html>
"""
    (site_dir / "index.html").write_text(html_text, encoding="utf-8")

